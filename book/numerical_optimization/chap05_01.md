---
title: Chapter5-01. The Linear Conjugate Grdient Method
book_title: Numerical Optimization
book_chapter: 5
book_section: 1
---

## 5.1 The Linear Conjugate Grdient Method

The problem we consider

#### Problem 1
* $A$,
    * $n \times n$ symmetrics positive definite matirx
* $b$,
    * $n$ vector

Find $x \in \mathbb{R}^{n}$ which satisfy

$$
\begin{equation}
    \min_{x}
        Ax = b
    .
    \label{euqation_05_01}
\end{equation}
$$


<div class="end-of-statement" style="text-align: right">■</div>

#### Remark
The solution of problem1 is euivalent to the following problem.

$$
\begin{eqnarray}
    \min_{x}
        \phi(x)
        :=
        \frac{1}{2}
            x^{\mathrm{T}}
            A
            x
        -
        b^{\mathrm{T}}x
    \label{euqation_05_02}
    .
\end{eqnarray}
$$

Indeed, since $A$ is positive symmetric definite, the solution of the quadratic problem uniquely exists.

$$
\begin{eqnarray}
    \nabla \phi(x)
    & = &
        Ax - b
    & =: &
    r(x)
    .
    \label{equation_05_03}
\end{eqnarray}
$$

$$
\begin{eqnarray}
    r_{k}
    & := &
        A x_{k} - b
    .
    \label{equation_05_04}
\end{eqnarray}
$$

<div class="end-of-statement" style="text-align: right">■</div>

#### Proposition
* $$\{p_{0}, \ldots, p_{l}\}$$,
    * conjugate with respocet to $A$,

Then $$\{p_{0}, \ldots, p_{l}\}$$ is linearly independent.

#### proof

<div class="QED" style="text-align: right">$\Box$</div>


#### Definition conjugate
* $l \in \mathbb{N}$,
* $$\{p_{0}, \ldots, p_{l}\}$$,
    * nonzero vectors
    * $p_{k} \in \mathbb{R}^{n}$,
* $A$,
    * $n \times n$ symmetric positive definite matrix

$$\{p_{0}, \ldots, p_{l}\}$$ is said to be conjugate with respect to $A$ if

$$
\begin{eqnarray}
    p_{i}^{\mathrm{T}}
    A
    p_{j}
    =
    0
    \quad
    (i \neq j)
    .
    \label{equation_05_05}
\end{eqnarray}
$$

<div class="end-of-statement" style="text-align: right">■</div>

#### Definition conjugate direction method
* $$\{p_{0}, \ldots, p_{n-1}\}$$,
* $x_{0} \in \mathbb{R}^{n}$,

Step0.  $k := 0$,

Step1.

$$
\begin{eqnarray}
    \alpha_{k}
    & := &
        -
        \frac{
            r_{k}^{\mathrm{T}}p_{k}
        }{
            p_{k}^{\mathrm{T}}
            A
            p_{k}
        }
    \nonumber
    \\
    & = &
        -
        \frac{
            (A x_{k} - b)^{\mathrm{T}}p_{k}
        }{
            p_{k}^{\mathrm{T}}
            A
            p_{k}
        }
    \label{equation_05_07}
\end{eqnarray}
$$

Step2.

$$
\begin{equation}
    x_{k + 1}
    :=
    x_{k}
    +
    \alpha_{k}
    p_{k}
    \label{equation_05_06}
\end{equation}
$$

Step3. $k \leftarrow k + 1$,

Step4. If $k = n$, it finishes the algorithm. Otherwise go to Step1.

<div class="end-of-statement" style="text-align: right">■</div>

#### Theorem 5.1
* $x_{0} \in \mathbb{R}^{n}$,
* $$\{x_{0}, \ldots, x_{n-1}\}$$,
    * The sequence generated by $$\{x_{0}, \ldots, x_{n-1}\}$$

The sequence covergexs to the solution of the linear system $$\eqref{equation_05_01}$$ in at most $n$ steps.

#### proof
Since $$\{p_{0}, \ldots, p_{l}\}$$ is linearly indepdent, there exist $$\sigma_{0}, \ldots, \sigma_{n-1}$$ such that

$$
\begin{eqnarray}
    x^{*} - x_{0}
    :=
    \sigma_{0} p_{0}
    +
    \cdots
    +
    \sigma_{n-1} p_{n-1}
    .
\end{eqnarray}
$$

By multiplying the equation by $p_{k}^{\mathrm{T}} A$ and using the conjugacy 

$$
\begin{eqnarray}
    & &
        x^{*} - x_{0}
        =
        \sigma_{0} p_{0}
        +
        \cdots
        +
        \sigma_{n-1} p_{n-1}
    \nonumber
    \\
    & \Leftrightarrow &
        p_{k}^{\mathrm{T}}A
        (x^{*} - x_{0)}
        =
        p_{k}^{\mathrm{T}}A
        \left(
            \sigma_{0} p_{0}
            +
            \cdots
            +
            \sigma_{n-1} p_{n-1}
        \right)
    \nonumber
    \\
    & \Leftrightarrow &
        p_{k}^{\mathrm{T}}A
        (x^{*} - x_{0)}
        =
        p_{k}^{\mathrm{T}} A
        \sigma_{k} p_{k}
    \nonumber
    \\
    & \Leftrightarrow &
        \sigma_{k}
        =
        \frac{
            p_{k}^{\mathrm{T}} A (x^{*} - x_{0})
        }{
            p_{k}^{\mathrm{T}}A p_{k}
        }
    \label{equation_05_08}
\end{eqnarray}
$$

$$
\begin{eqnarray}
    & &
        x_{k}
        =
        x_{k-1}
        +
        \alpha_{k-1}
        p_{k-1}
    \nonumber
    \\
    & \Leftrightarrow &
        x_{k}
        =
        x_{0}
        +
        p_{1}
        x_{1}
        +
        \cdots
        +
        \alpha_{k-1}
        p_{k-1}
    \nonumber
    \\
    & \Leftrightarrow &
        x_{k}
        -
        x_{0}
        =
        p_{1}
        x_{1}
        +
        \cdots
        +
        \alpha_{k-1}
        p_{k-1}
    \nonumber
    \\
    & \Leftrightarrow &
        p_{k}^{\mathrm{T}}A
        (x_{k} - x_{0})
        =
        0
    .
\end{eqnarray}
$$

Hence

$$
\begin{eqnarray}
    p_{k}^{\mathrm{T}}A
    (x^{*} - x_{0})
    & = &
        p_{k}^{\mathrm{T}}A
        (x^{*} - x_{k} + x_{k} - x_{0})
    \nonumber
    \\
    & = &
        p_{k}^{\mathrm{T}}A
        (x^{*} - x_{k})
    \nonumber
    \\
    & = &
        p_{k}^{\mathrm{T}}
        (b - Ax_{k})
    \nonumber
    \\
    & = &
        -
        p_{k}^{\mathrm{T}}
        r_{k}
    .
    \nonumber
\end{eqnarray}
$$

Therefore, $\sigma_{k} = \alpha_{k}$.

<div class="QED" style="text-align: right">$\Box$</div>

#### Theorem 5.2
* $x_{0} \in \mathbb{R}^{n}$,
* $$\{x_{0}, \ldots, x_{n}\}$$,
    * The sequence generated by the conjugate direction algorithm

Then

$$
\begin{equation}
    \forall i = 0, \ldots, k-1,
    \
    r_{k}^{\mathrm{T}}p_{i}
    =
    0
    \label{equation_05_11}
\end{equation}
    .
$$

$x_{k}$ is the minimizer of $\phi(x)$ over set

$$
\begin{eqnarray}
    D_{k}
    :=
    \{
        x
        \mid
        x = x_{0} + \mathrm{span}\{p_{0}, \ldots, p_{k-1}\}
    \}
    .
    \label{equation_05_12}
\end{eqnarray}
$$

#### proof
We first show that a point $\tilde{x}$ minimizes $\phi$ over the set if and only if

$$
    \forall, i = 0, \ldots, k -1,
    r(\tilde{x})^{\mathrm{T}}
    p_{i}
    =
    0
    .
$$

Indeed, let us define

$$
    h(\sigma)
    :=
    \phi(x_{0} + \sigma_{0}p_{0} + \cdots + \sigma_{k-1}p_{k-1})
    .
$$

Since composition of convex functions is also convex, $h$ is a strictly convex function.
It has a unique minimizer $\sigma^{*}$ that satisfies

$$
\begin{eqnarray}
    \forall i = 0, 1, \ldots, k-1,
    \
    & &
        \frac{
            \partial h(\sigma^{*})
        }{
            \partial \sigma_{i}
        }
        =
        0
    \nonumber
    \\
    & \Leftrightarrow &
        \nabla
        \phi(x_{0} + \sigma_{0}^{*}p_{0} + \cdots + \sigma_{k-1}^{*}p_{k-1})
        p_{i}
        =
        0
    \nonumber
    \\
    & \Leftrightarrow &
        r(\tilde{x})
        p_{i}
        =
        0
    \nonumber
\end{eqnarray}
    .
$$

We use the induction to prove the statement.
For the case $k = 1$,

$$
\begin{eqnarray}
    r_{1}^{\mathrm{T}}p_{0}
    & = &
        (Ax_{1} - b)^{\mathrm{T}}
        p_{0}
    \nonumber
    \\
    & = &
        (Ax_{0} + A\alpha_{0}p_{0} - b)^{\mathrm{T}}
        p_{0}
    \\
    & = &
        (x_{0}^{\mathrm{T}}A + \alpha_{0}p_{0}^{\mathrm{T}}A - b^{\mathrm{T}})
        p_{0}
    \\
    & = &
        x_{0}^{\mathrm{T}}A
        p_{0}
        +
        \alpha_{0}p_{0}^{\mathrm{T}}A
        p_{0}
        -
        b^{\mathrm{T}} p_{0}
    \nonumber
    \\
    & = &
        x_{0}^{\mathrm{T}}A
        p_{0}
        +
        r_{0}^{\mathrm{T}}p_{0}
        -
        b^{\mathrm{T}} p_{0}
    \nonumber
    \\
    & = &
        r_{0}^{\mathrm{T}}
        p_{0}
        +
        r_{0}^{\mathrm{T}}p_{0}
    \nonumber
    \\
    & = &
        0
    .
    \nonumber
\end{eqnarray}
$$

Let us assume that the statement holds up to $k = l$.
For the case $k = l + 1$,

$$
\begin{eqnarray}
    r_{k+1}
    & = &
        Ax_{k+1} - b
    \nonumber
    \\
    & = &
        Ax_{k}
        +
        A\alpha_{k}p_{k}
        - b
    \nonumber
    \\
    & = &
        r_{k}
        +
        A\alpha_{k}p_{k}
    .
    \nonumber
\end{eqnarray}
$$

Hence

$$
\begin{eqnarray}
    p_{k}^{\mathrm{T}}r_{k+1}
    & = &
        p_{k}^{\mathrm{T}}
        \left(
            r_{k}
            +
            A\alpha_{k}p_{k}
        \right)
    \nonumber
    \\
    & = &
        p_{k}^{\mathrm{T}}
        r_{k}
        -
        r_{k}^{\mathrm{T}}
        p_{k}
    \nonumber
    \\
    & = &
        0
    .
    \nonumber
\end{eqnarray}
$$

By conjugacy and the assumption of the induction, for all $i = 0, \ldots, k-1$,

$$
\begin{eqnarray}
    p_{i}^{\mathrm{T}}r_{k+1}
    & = &
        p_{i}^{\mathrm{T}}
        \left(
            r_{k}
            +
            A\alpha_{k}p_{k}
        \right)
    \nonumber
    \\
    & = &
        p_{i}^{\mathrm{T}}
        A\alpha_{k}p_{k}
    \nonumber
    \\
    & = &
        0
    .
    \nonumber
\end{eqnarray}
$$

<div class="QED" style="text-align: right">$\Box$</div>

#### Algorithm 5.1 CG-Preliminary Version
* $x_{0} \in \mathbb{R}^{n}$,



$$
\begin{eqnarray}
    \alpha_{k}
    & \leftarrow  &
        -
        \frac{
            r_{k}^{\mathrm{T}}p_{k}
        }{
            p_{k}^{\mathrm{T}}Ap_{k}
        }
        \label{equation_05_14_a}
    \\
    x_{k+1}
    & \leftarrow  &
        x_{k}
        +
        \alpha_{k} p_{k}
        \label{equation_05_14_b}
    \\
    r_{k+1}
    & \leftarrow  &
        A x_{k+1} - b
        \label{equation_05_14_c}
    \\
    \beta_{k+1}
    & \leftarrow  &
        \frac{
            r_{k+1}^{\mathrm{T}} A p_{k}
        }{
            p_{k}^{\mathrm{T}}A p_{k}
        }
        \label{equation_05_14_d}
    \\
    p_{k+1}
    & \leftarrow  &
        - r_{k+1}
        +
        \beta_{k+1} p_{k}
        \label{equation_05_14_e}
    \\
    k
    & \leftarrow  &
        k + 1
        \label{equation_05_14_f}
\end{eqnarray}
$$

<div class="end-of-statement" style="text-align: right">■</div>


#### Definition Krylov subspace
Krylov subspace of degree $k$ for $r_{0}$ is defined as

$$
    \mathcal{K}(r_{0}; k)
    :=
    \mathrm{span}\{
        r_{0}, A r_{0}, \ldots, A^{k}r_{0}
    \}
    .
$$

<div class="end-of-statement" style="text-align: right">■</div>


#### Theorem 5.3
* $$\{x_{k}\}$$
    * the seqeunce generated by conjugate gradient method

$$
\begin{eqnarray}
    r_{k}^{\mathrm{T}}r_{i}
    & = &
        0
    \label{equation_05_16}
    \\
    \mathrm{span}\{r_{0}, \ldots, r_{k}\}
    & = &
        \mathrm{span}\{r_{0}, \ldots, A^{k}r_{0}\}
    \label{equation_05_17}
    \\
    \mathrm{span}\{p_{0}, \ldots, p_{k}\}
    & = &
        \mathrm{span}\{r_{0}, \ldots, A^{k}r_{0}\}
    \label{equation_05_17}
    \\
    p_{k}^{\mathrm{T}}Ap_{i}
    & = &
        0
    \label{equation_05_17}
\end{eqnarray}
$$


#### proof

<div class="QED" style="text-align: right">$\Box$</div>
