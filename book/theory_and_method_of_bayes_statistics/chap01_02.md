---
title: Chapter1. Mathematical background
book_title: Theory and method of Bayes Statistics
book_chapter: 1
book_section: 2
---

## 1.2 Metrics


### 1.2.1 Partition function and free energy

$$
\begin{eqnarray}
    \int_{\mathbb{R}}
    \ dx_{1}
    \int_{\mathbb{R}}
    \ dx_{2}
    \cdots
    \int_{\mathbb{R}}
    \ dx_{n}
    Z_{n}(x^{n}; 1)
    & = &
        \int_{\mathbb{R}}
        \ dx_{1}
        \int_{\mathbb{R}}
        \ dx_{2}
        \cdots
        \int_{\mathbb{R}}
        \ dx_{n}
        \left(
            \int_{\mathbb{R}}
                \varphi(w)
            \ dw
            \prod_{i = 1}^{n}
                p(x_{i} \mid w)
        \right)
    \nonumber
    \\
    & = &
        \int_{\mathbb{R}}
            \varphi(w)
        \ dw
        \int_{\mathbb{R}}
        \ dx_{1}
        \int_{\mathbb{R}}
        \ dx_{2}
        \cdots
        \int_{\mathbb{R}}
        \ dx_{n}
        \left(
            \prod_{i = 1}^{n}
                p(x_{i} \mid w)
        \right)
    \nonumber
    \\
    & = &
        \int_{\mathbb{R}}
            \varphi(w)
        \ dw
        \prod_{i = 1}^{n}
        \left(
            \int_{\mathbb{R}}
            \ dx_{i}
            p(x_{i} \mid w)
        \right)
    \nonumber
    \\
    & = &
        \int_{\mathbb{R}}
            \varphi(w)
        \ dw
    \nonumber
    \\
    & = &
        1
    .
    \nonumber
\end{eqnarray}
$$

By our assumption (even though not clearly mentioned in the book), true distribution of $X^{n}$ is given by

$$
    q_{X^{n}}
    (X^{n})
    =
    \prod_{i = 1}^{n}
        q(X_{i})
    .
$$

On the other hand, $Z_{n}(X^{n}; 1)$ is the distribution of $X^{n}$ estimated by our statistical model and our prior distribution.
That is

$$
\begin{eqnarray}
    p(x^{n})
    & := &
        Z_{n}(x^{n}; 1)
    \nonumber
    \\
    & = &
        \int_{W}
            \varphi(w)
            \prod_{i=1}^{n}
                p(x_{i} \mid w)
        \ dw
    \nonumber
    \\
    & = &
        \int_{W}
            \varphi(w)
            \prod_{i=1}^{n}
                \frac{
                    p(x_{i}, w)
                }{
                    \varphi(w)
                }
        \ dw
    \nonumber
    \\
    & = &
        \prod_{i=1}^{n}
            p(x_{i})
    \nonumber
\end{eqnarray}
$$

#### Definition. free energy
Let

$$
    F_{n}(\beta)
    :=
    -
    \frac{1}{\beta}
    \log Z_{n}(\beta)
    .
$$

$F_{n}(\beta)$ is called free energy.
In particular, if $\beta = 1$, free energy is equal to the additive inverse of the logarithmic likelihood:

$$
    \log Z_{n}(X^{n}; 1)
    .
$$

<div class="end-of-statement" style="text-align: right">■</div>

#### Definition. Entropy
* $q$,
    * true dsitribution

$$
    S
    :=
    -
    \int_{\mathbb{R}}
        q(x)
        \log q(x)
    \ dx
    .
$$

<div class="QED" style="text-align: right">$\Box$</div>

#### Definition emprical entropy
Let

$$
    S_{n}
    :=
    -
    \frac{1}{n}
    \sum_{i=1}^{n}
        \log q(X_{i})
    .
$$

$S_{n}$ is called emrical entropy.

<div class="end-of-statement" style="text-align: right">■</div>

By definition,

$$
\begin{eqnarray}
    \mathrm{E}
    \left[
        S_{n}
    \right]
    & = &
        S
    \label{equation_01_16}
    \\
    F_{n}(X^{n}; 1)
    & = &
        - \log Z_{n}(X^{n}; 1)
    \nonumber
    \\
    & = &
        - \log
        \left(
            \int_{W}
                \varphi(w)
                \prod_{i=1}^{n}
                    p(X_{i} \mid w)
            \ dw
        \right)
    \nonumber
    \\
    & = &
        - \log
        \left(
            \int_{W}
                \varphi(w)
                \prod_{i=1}^{n}
                    \frac{
                        p(X_{i}, w)
                    }{
                        \varphi(w)
                    }
            \ dw
        \right)
    \nonumber
    \\
    & = &
        - \log
        \left(
            \int_{W}
                \prod_{i=1}^{n}
                    p(X_{i}, w)
            \ dw
        \right)
    \nonumber
    \\
    & = &
        - \log
        \left(
            \prod_{i=1}^{n}
            p(X_{i})
        \right)
    \nonumber
    \\
    & = &
        -
        \log
        \left(
            \prod_{i=1}^{n}
                q(X_{i})
        \right)
        +
        \log
        \left(
            \prod_{i=1}^{n}
                q(X_{i})
        \right)
        -
        \log
        \left(
            \prod_{i=1}^{n}
                p(X_{i})
        \right)
    \nonumber
    \\
    \mathrm{E}
    \left[
        F_{n}(X^{n}; 1)
    \right]
    & = &
        n
        S
        +
        \int_{\mathbb{R}^{N}}
        \cdots
        \int_{\mathbb{R}^{N}}
            \log
            \left(
                \frac{
                    \prod_{i=1}^{n}
                        q(x_{i})
                }{
                \prod_{i=1}^{n}
                    p(x_{i})
                }
            \right)
            \prod_{i=1}^{n}
                q(x_{i})
        \ dx_{1}
        \cdots
        \ dx_{n}
    \nonumber
\end{eqnarray}
$$

The first term is the entropy of true distribution.
The second term is the Kullback-Leibler divergence from $ \prod_{i=1}^{n} q(x_{i}) $ to $ \prod_{i=1}^{n} p(x_{i}) $.


### 1.2.2 Estimation and Generalization

#### Definition. Generalization losses
Let

$$
\begin{eqnarray}
    G_{n}(X^{n})
    & := &
        -
        \int_{\mathbb{R}^{n}}
            q(x)
            \log p^{*}(x \mid X^{n})
        \ dx
    \nonumber
    \\
    & = &
        \int_{\mathbb{R}^{n}}
            -
            q(x)
            \log
                q(x)
            +
            q(x)
            \log
                q(x)
            -
            q(x)
            \log
                p^{*}(x \mid X^{n})
        \ dx
    \nonumber
    \\
    & = &
        S
        +
        \int_{\mathbb{R}^{n}}
            q(x)
            \log
                \frac{
                    q(x)
                }{
                    p^{*}(x \mid X^{n})
                }
        \ dx
    \label{equation_01_22}
\end{eqnarray}
$$

where $p^{*}$ is prediction density function.
$G_{n}(X^{n})$ is called generalization losses.

<div class="end-of-statement" style="text-align: right">■</div>

#### Definition. training losses
Let

$$
\begin{eqnarray}
    T_{n}
    & := &
        - \frac{ 1 }{ n }
        \sum_{i=1}^{n}
            \log p(X_{i} \mid X^{n})
    \nonumber
    \\
    & = &
        - \frac{ 1 }{ n }
        \sum_{i=1}^{n}
            \log p^{*}(X_{i} \mid X^{n})
    \nonumber
\end{eqnarray}
$$

where $p^{*}$ is prediction density function.
$T_{n}$ is called traning losses.

<div class="end-of-statement" style="text-align: right">■</div>

* In practical, it is hard to calculate $G_{n}$ because $G_{n}$ depends on the true distribution $q$ which will never be known,
* On the other hand, $T_{n}(\omega)$ is observable,

Here we will consider the problem: how $T_{n}$ approximates $G_{n}$.

$$
\begin{eqnarray}
    p(X_{n + 1} \mid X^{n})
    & = &
        \frac{
            p(X_{n + 1}, X^{n})
        }{
            p(X^{n})
        }
    \nonumber
    \\
    & = &
        \frac{
            \int_{W}
                p(X_{n + 1}, X^{n} \mid w)
                \varphi(w)
            \ dw
        }{
            Z_{n}(X^{n}; 1)
        }
    \nonumber
    \\
    & = &
        \frac{
            \int_{W}
                p(X_{n + 1} \mid w)
                p(X^{n} \mid w)
                \varphi(w)
            \ dw
        }{
            Z_{n}(X^{n}; 1)
        }
        \quad
        (\because \text{conditional independence})
    \nonumber
    \\
    & = &
        \frac{
            \int_{W}
                p(X_{n + 1} \mid w)
                \prod_{i = 1}^{n}
                    p(X_{i} \mid w)
                \varphi(w)
            \ dw
        }{
            Z_{n}(X^{n}; 1)
        }
    \nonumber
    \\
    & = &
        \frac{
            Z_{n + 1}(X^{n + 1}; 1)
        }{
            Z_{n}(X^{n}; 1)
        }
    \nonumber
\end{eqnarray}
$$

Note that if we assume I.I.D. of $X_{1}, \ldots, X_{n + 1}$, $p(X_{n + 1} \mid X^{n}) = p(X_{n + 1})$.

$$
\begin{eqnarray}
    -
    \log
        p^{*}(X_{n + 1} \mid X^{n})
    & = &
        -
        \log
            p^{*}(X_{n + 1} \mid X^{n})
        \quad
        (\because \text{By definition. See } \eqref{equation_01_09})
    \nonumber
    \\
    & = &
        -
        \log
            Z_{n + 1}(X^{n + 1}; 1)
        +
        \log
            Z_{n}(X^{n}; 1)
    \nonumber
    \\
    & = &
        F_{n + 1}(X^{n+1}; 1)
        -
        F_{n}(X^{n}; 1)
    \nonumber
\end{eqnarray}
$$

Taking integral of the both sides,

$$
\begin{eqnarray}
    & &
        \int
        \cdots
        \int
            -\log
                p^{*}(x^{n + 1} \mid x^{n})
            \prod_{i=1}^{n+1}
                q(x_{i})
        \ dx_{1}
        \cdots
        \ dx_{n + 1}
        =
        \int
        \cdots
        \int
            F_{n+1}(x^{n+1}; 1)
            \prod_{i=1}^{n+1}
                q(x_{i})
        \ dx_{1}
        \cdots
        \ dx_{n + 1}
        +
        \int
        \cdots
        \int
            F_{n}(x^{n}; 1)
            \prod_{i=1}^{n}
                q(x_{i})
        \ dx_{1}
        \cdots
        \ dx_{n}
    \nonumber
    \\
    & \Leftrightarrow &
        \int
        \cdots
        \int
            G_{n}(x^{n})
            \prod_{i=1}^{n}
                q(x_{i})
        \ dx_{1}
        \cdots
        \ dx_{n + 1}
        =
        \mathrm{E}
        \left[
            F_{n+1}(X^{n+1}; 1)
        \right]
        +
        \mathrm{E}
        \left[
            F_{n}(X^{n}; 1)
        \right]
    \nonumber
    \\
    & \Leftrightarrow &
        \mathrm{E}
        \left[
            G_{n}(X^{n})
        \right]
        =
        \mathrm{E}
        \left[
            F_{n+1}(X^{n+1}; 1)
        \right]
        +
        \mathrm{E}
        \left[
            F_{n}(X^{n}; 1)
        \right]
    .
    \nonumber
\end{eqnarray}
$$

### 1.2.3. Computable example
In Baysian estimation, it is hard to calculate posterior distribution or prediction density function analytically in many cases.

#### Definition exponential family
* $v: \mathbb{R}^{N} \rightarrow \mathbb{R}$,
* $f: W \rightarrow \mathbb{R}^{J}$,
* $g: \mathbb{R}^{N} \rightarrow \mathbb{R}^{J}$,
* $J \in \mathbb{N}$,

Statistical model $p(x \mid w)$ is said to be exponential family with $v, f, g$ if

$$
    p(x \mid w)
    =
    v(x)
    \exp
    \left(
        f(w)^{\mathrm{T}}
        g(x)
    \right)
    .
$$

<div class="end-of-statement" style="text-align: right">■</div>

Since

$$
    \int_{\mathbb{R}^{N}}
        p(x \mid w)
    \ dx
    =
    1,
$$

$$
    \int_{\mathbb{R}^{N}}
        v(x)
        \exp
        \left(
            f(w)^{\mathrm{T}}
            g(x)
        \right)
    \ dx
    .
$$

Suppose that prior distribution is given by

$$
    \varphi(w \mid \varphi)
$$

