---
title: Chapter1. Mathematical background
book_title: Theory and method of Bayes Statistics
book_chapter: 1
book_section: 1
---

## 1.1 The definition of bayesian estimation

* $(\Omega, \mathcal{F}, P)$,
    * probability sp.
* $N \in \mathbb{N}$,
    * the dimention of the inputs
* $n \in \mathbb{N}$,
    * the number of input data
* $x_{i} \in \mathbb{R}^{N}$ $(i = 1, \ldots, n)$,
    * we call $$\{x_{i}\}_{i=1, \ldots, n}$$ a sample,
    * $x_{i}$ is the actual observed data
* $x^{n} := (x_{1}, \ldots, x_{n})$,
* $q$,
    * distribution function over $(\mathbb{R}^{N})^{n}$,
    * or the p.d.f. of the distirbution
* $X^{n} = (X_{1}, \ldots, X_{n})$,
    * the r.v.s whose distribution is $q$,


#### Remark 2
In statitstics, estimating true distribution based on samples is called statistical inference/estimation.
However, in information theoyr, the estimation is called statistical learning.

<div class="end-of-statement" style="text-align: right">â– </div>

Let $f: (\mathbb{R}^{N})^{n} \rightarrow \mathbb{R}$.

$$
    \mathrm{E}
    \left[
        f(X^{n})
    \right]
    =
    \int_{\mathbb{R}^{N}}
    \cdots
    \int_{\mathbb{R}^{N}}
        f(x_{1}, \ldots, x_{n})
    \
    \prod_{i=1}^{n}
        q(x)
        dx_{i}
    .
$$

* $d \in \mathbb{N}$,
    * the dimension of the parameters of given model
* $W \subseteq \mathbb{R}^{d}$,
    * parameter space
* $\Theta$,
    * r.v. of parameter
* $p(x \mid w)$,
    * the p.d.f. of $X$ given $\Theta = w$,
    * statistical model
* $\phi: W \rightarrow \mathbb{R}$,
    * the p.d.f. of $\Theta$,
    * prior distribution
* $\beta \in \mathbb{R}_{> 0}$,
    * constant
    * inverse temperature

$p(x \mid w)$ is the p.d.f. of

$$
    \mathrm{E}
    \left[
        X
        \mid
        \Theta = w
    \right]
    .
$$

$p(w \mid X^{n})$ is the p.d.f. of

$$
    \mathrm{E}
    \left[
        \Theta
        \mid
        X^{n}
    \right]
    .
$$

Posterior distribution with inverse temparature $\beta$ is defined by

$$
\begin{eqnarray}
    p(w \mid X^{n})
    & := &
        \frac{
            1
        }{
            Z_{n}(X^{n}; \beta)
        }
        \phi(w)
        \prod_{i=1}^{n}
            p(X_{i} \mid w)^{\beta}
    \nonumber
    \\
    Z_{n}(X^{n}; \beta)
    & := &
        \int_{W}
            \phi(w)
            \prod_{i=1}^{n}
                p(X_{i} \mid w)^{\beta}
        \ dw
    \nonumber
\end{eqnarray}
$$

$Z_{n}(X^{n}; \beta)$ is called the partition function.
If $\beta=1$, $Z_{n}(1)$ is called the marginal likelihood.

$$
\begin{eqnarray}
    \mathrm{E}_{w}
    \left[
        f(w)
    \right]
    & := &
        \int_{W}
            f(w)
            p(w \mid X^{n})
        \ dw
    \label{equation_01_07}
    \\
    & = &
        \mathrm{E}
        \left[
            f(\Theta)
            \mid
            X^{n}
        \right]
    \nonumber
\end{eqnarray}
$$

The predictive density function is defined by

$$
\begin{eqnarray}
    p^{*}(x)
    & := &
        p(x \mid X^{n})
    \nonumber
    \\
    & := &
        \mathrm{E}_{w}
        \left[
            p(x \mid w)
        \right]
    \nonumber
    \\
    & = &
        \int_{W}
            p(x \mid w)
        \ p(w \mid X^{n})
        \ dw
    .
    \label{equation_01_08}
\end{eqnarray}
$$


